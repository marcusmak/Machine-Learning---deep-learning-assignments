{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## Part 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "imdbId  Action  Adventure  Animation  Comedy  Drama  Horror  Romance\n0   986264       0          0          0       0      1       0        0\n1  1379182       0          0          0       0      1       0        0\n2   361748       0          1          0       0      1       0        0\n3  4175888       0          0          0       0      0       1        0\n4   284445       1          1          0       0      0       1        0\n"
    }
   ],
   "source": [
    "from skimage import io , transform, color\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "part1_path = './patched_pa2_data/pa2_data 2/part1_data/'\n",
    "# sub_set = pd.read_csv(\"./patched_pa2_data/pa2_data 2/part1_data/submission.csv\")\n",
    "# test_set = pd.read_csv(part1_path + \"test.csv\")\n",
    "train_df = pd.read_csv(part1_path + \"train.csv\")\n",
    "print(train_df.head())\n",
    "class posterDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_path, dataFrame, transform = None):\n",
    "        # self.file_path = file_path\n",
    "        # self.classes = [0, 1]\n",
    "        self.root_path = root_path\n",
    "        self.transform = transform\n",
    "        # Some preprocessing\n",
    "   \n",
    "        self.df = dataFrame\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = self.root_path + 'images/' + str(self.df.iloc[idx,0]) + '.jpg'\n",
    "        X = io.imread(img_name)\n",
    "        y = np.asarray(self.df.iloc[idx, 1:], dtype = np.float32)\n",
    "        \n",
    "        if(len(X.shape) == 2):\n",
    "            X = color.grey2rgb(X)\n",
    "            \n",
    "        sample = {\"image\" : X , \"genre\" : y}\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "    \n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "# img_names = file_name(train_set['imdbId'].apply(str))\n",
    "# plt.imshow(io.imread(part1_path + 'images/118589.jpg'))\n",
    "# io.imread(part1_path + 'images/118589.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "10365"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),    # range [0, 255] -> [0.0,1.0]\n",
    "     transforms.Normalize((0.5, ), (0.5, ))])  \n",
    "full_set = posterDataset(part1_path,train_df,transform) \n",
    "len(full_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(full_set))\n",
    "train_set, val_set = random_split(full_set, [train_size, len(full_set)-train_size ])\n",
    "# _,_,train_set, val_set = random_split(full_set, [train_size, len(full_set)-train_size-110, 100 ,10 ]\n",
    "# full_set.df[full_set.df['imdbId'] == 2093997]\n",
    "# print(len(full_set))\n",
    "# full_set[5718]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #268 x 182 x 3\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)  #264 x 178 x 6\n",
    "        self.pool = nn.MaxPool2d(2, 2) #132 x 89 x 6\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=10,  kernel_size=7)  #126 x 83 x 10\n",
    "        self.conv3 = nn.Conv2d(in_channels=10, out_channels=16,  kernel_size=11)  #53 x 31 x 16\n",
    "        self.fc1 = nn.Linear(6240, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  3, 32, 32\n",
    "        # out_dim = in_dim - kernel_size + 1  \n",
    "        x = self.pool(F.relu(self.conv1(x))) #6, 14, 14 \n",
    "        x = self.pool(F.relu(self.conv2(x))) #16, 5, 5\n",
    "        x = self.pool(F.relu(self.conv3(x))) #16, 5, 5\n",
    "        x = x.view(-1, 6240)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1          [-1, 6, 264, 178]             456\n         MaxPool2d-2           [-1, 6, 132, 89]               0\n            Conv2d-3          [-1, 10, 126, 83]           2,950\n         MaxPool2d-4           [-1, 10, 63, 41]               0\n            Conv2d-5           [-1, 16, 53, 31]          19,376\n         MaxPool2d-6           [-1, 16, 26, 15]               0\n            Linear-7                  [-1, 120]         748,920\n            Linear-8                   [-1, 84]          10,164\n            Linear-9                    [-1, 7]             595\n================================================================\nTotal params: 782,461\nTrainable params: 782,461\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.56\nForward/backward pass size (MB): 3.93\nParams size (MB): 2.98\nEstimated Total Size (MB): 7.48\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = Net()\n",
    "summary(model, input_size=(3,268, 182))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import logging\n",
    "def save_checkpoint(save_path, model, optimizer, val_loss):\n",
    "    if save_path==None:\n",
    "        return\n",
    "    save_path = save_path \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'val_loss': val_loss}\n",
    "\n",
    "    torch.save(state_dict, save_path)\n",
    "\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(model, optimizer):\n",
    "    save_path = f'cifar_net.pt'\n",
    "    state_dict = torch.load(save_path)\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    val_loss = state_dict['val_loss']\n",
    "    print(f'Model loaded from <== {save_path}')\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "\n",
    "def TRAIN(net, train_loader, valid_loader,  num_epochs, eval_every, total_step, criterion, optimizer, val_loss, device, save_name):\n",
    "    train_loss_his = []\n",
    "    val_loss_his = []\n",
    "    running_loss = 0.0\n",
    "    # running_corrects = 0\n",
    "    # running_num = 0\n",
    "    global_step = 0\n",
    "    if val_loss == None:\n",
    "        best_val_loss = float(\"Inf\")  \n",
    "    else: \n",
    "        best_val_loss = val_loss\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        for i, samples in enumerate(train_loader):\n",
    "            inputs = samples['image']\n",
    "            labels = samples['genre']\n",
    "            net.train()                             # set to trainning mode\n",
    "            inputs = inputs.to(device)              \n",
    "            labels = labels.to(device)\n",
    "            '''Training of the model'''\n",
    "            # Forward pass\n",
    "            outputs = net(inputs)\n",
    "            # _, preds = torch.max(outputs.data, 1)   # get the predicted output val,index\n",
    "\n",
    "\n",
    "            loss = criterion(outputs, labels)       # calc the loss\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()                   \n",
    "            loss.backward()                         # calc grad\n",
    "            optimizer.step()                        # refreash the weight\n",
    "            global_step += 1\n",
    "\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            # running_corrects += torch.sum(preds == labels.data)\n",
    "            # running_num += len(labels)\n",
    "            \n",
    "            '''Evaluating the model every x steps'''\n",
    "            if global_step % eval_every == 0:\n",
    "                with torch.no_grad():\n",
    "                    net.eval()\n",
    "                    val_running_loss = 0\n",
    "                    \n",
    "                    # val_running_corrects = 0\n",
    "                    for samples in valid_loader:\n",
    "                            val_inputs = samples['image']\n",
    "                            val_labels = samples['genre']\n",
    "                            val_outputs = net(val_inputs)\n",
    "                            val_loss = criterion(val_outputs, val_labels)\n",
    "                            # _, preds = torch.max(val_outputs.data, 1)\n",
    "                            val_running_loss += val_loss.item()\n",
    "\n",
    "                            # val_running_corrects += torch.sum(preds == val_labels.data)\n",
    "\n",
    "\n",
    "                    average_train_loss = running_loss / eval_every\n",
    "                    average_val_loss = val_running_loss / len(valid_loader)\n",
    "                    # average_train_acc = running_corrects / float(running_num)\n",
    "                    # average_val_acc = val_running_corrects / float(len(valid_loader))\n",
    "                    train_loss_his.append(average_train_loss)\n",
    "                    val_loss_his.append(average_val_loss)\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'.format(epoch+1, num_epochs, global_step, total_step, average_train_loss,average_val_loss))\n",
    "\n",
    "                    running_loss = 0.0\n",
    "                    running_num = 0\n",
    "                    # running_corrects = 0\n",
    "                    \n",
    "                    if average_val_loss < best_val_loss:\n",
    "                        best_val_loss = average_val_loss\n",
    "                        save_checkpoint(save_name, net, optimizer, best_val_loss)\n",
    "                    \n",
    "                    \n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cpu\n"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "eval_every = 10\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=200,\n",
    "                                          shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val_set, batch_size=200,\n",
    "                                          shuffle=True)\n",
    "\n",
    "total_step = len(train_loader)*num_epochs\n",
    "best_val_loss = None\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters() , lr=0.001)\n",
    "save_path = f'cifar_net.pt'\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "TRAIN(model, train_loader, valid_loader, num_epochs, eval_every, total_step, criterion, optimizer, best_val_loss, device, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([10, 7])"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "model(iter(valid_loader).next()['image']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}