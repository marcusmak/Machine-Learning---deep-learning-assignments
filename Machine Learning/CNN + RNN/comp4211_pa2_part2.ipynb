{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "comp4211_pa2_part2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW_doHjSIfjR",
        "colab_type": "code",
        "outputId": "e7b1468b-0bd1-4544-9ca8-adedc3343422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11fpe-l1lxkn",
        "colab_type": "text"
      },
      "source": [
        "##Some Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gysqHvWIJILS",
        "colab_type": "code",
        "outputId": "e10758ba-05ca-463e-8d65-4ce7efdee032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from skimage import io , transform, color\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZHJoLGFJhM-",
        "colab_type": "code",
        "outputId": "543ca0aa-4d0b-492d-a7b6-2c7582fb13d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "# part2_path = './patched_pa2_data/pa2_data 2/part2_data/'\n",
        "part2_path = '/content/drive/My Drive/Colab/patched_pa2_data/pa2_data 2/part2_data/'\n",
        "print(part2_path + \"train.csv\")\n",
        "train_df = pd.read_csv(part2_path + \"train.csv\")\n",
        "val_df = pd.read_csv(part2_path + \"validation.csv\")\n",
        "test_df = pd.read_csv(part2_path + \"test.csv\")\n",
        "sub_df = pd.read_csv(part2_path + \"submission.csv\")\n",
        "print(train_df.iloc[0])\n",
        "print(val_df.iloc[0])\n",
        "print(test_df.iloc[0])\n",
        "print(sub_df.iloc[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab/patched_pa2_data/pa2_data 2/part2_data/train.csv\n",
            "label                                                    1\n",
            "text     I guess those who have been in a one-sided rel...\n",
            "Name: 0, dtype: object\n",
            "label                                                    1\n",
            "text     This is one of the movies of Dev Anand who gav...\n",
            "Name: 0, dtype: object\n",
            "label                                                    1\n",
            "text     This movie is intelligent. That is, more than ...\n",
            "Name: 0, dtype: object\n",
            "label                                                    3\n",
            "text     Less self-conscious and much less pretentious ...\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yXsavh4Jptm",
        "colab_type": "code",
        "outputId": "54cb8c56-80c6-4e74-d402-e82ddd93270a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "train_df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>I guess those who have been in a one-sided rel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Almost 30 years later I recall this original P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>This movie is one of the only historical docum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>This movie has made me upset! When I think of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>I remember this movie when i was 13 seems a lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39995</th>\n",
              "      <td>0</td>\n",
              "      <td>Calling this a Sunday School movie might be ge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39996</th>\n",
              "      <td>1</td>\n",
              "      <td>Convicts is very much a third act sort of film...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39997</th>\n",
              "      <td>1</td>\n",
              "      <td>In the Mood for Love a teasing allegory of lon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39998</th>\n",
              "      <td>0</td>\n",
              "      <td>The fallen ones falls under the waste of life ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39999</th>\n",
              "      <td>0</td>\n",
              "      <td>If you have read the book - do not set your ho...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       label                                               text\n",
              "0          1  I guess those who have been in a one-sided rel...\n",
              "1          1  Almost 30 years later I recall this original P...\n",
              "2          1  This movie is one of the only historical docum...\n",
              "3          0  This movie has made me upset! When I think of ...\n",
              "4          0  I remember this movie when i was 13 seems a lo...\n",
              "...      ...                                                ...\n",
              "39995      0  Calling this a Sunday School movie might be ge...\n",
              "39996      1  Convicts is very much a third act sort of film...\n",
              "39997      1  In the Mood for Love a teasing allegory of lon...\n",
              "39998      0  The fallen ones falls under the waste of life ...\n",
              "39999      0  If you have read the book - do not set your ho...\n",
              "\n",
              "[40000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzZ_5Y5JKiTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class TabularDataset(Dataset):\n",
        "  \n",
        "#     def __init__(self, dataFrame, trainer = False, ):\n",
        "#         # self.file_path = file_path\n",
        "#         # self.classes = [0, 1]\n",
        "#         self.df = dataFrame\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         if torch.is_tensor(idx):\n",
        "#             idx = idx.tolist()\n",
        "#         X = self.df.iloc[idx,1]\n",
        "#         y = self.df.iloc[idx, 0]\n",
        "#         return X , y\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "# train_set = TabularDataset(train_df)\n",
        "# val_set = TabularDataset(val_df)\n",
        "# test_set = TabularDataset(test_df)\n",
        "# sub_set = TabularDataset(sub_df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ri8LCAWLb_E",
        "colab_type": "code",
        "outputId": "4434f1a1-17b7-4705-9684-5b1ddaa5847a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from torchtext import *\n",
        "from torchtext.data import *\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lseen8iJHCsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "txt_field = data.Field(tokenize=word_tokenize, lower=True, include_lengths=True, batch_first=True)\n",
        "label_field = data.Field(sequential=False, use_vocab=False, batch_first=True)\n",
        "\n",
        "# make splits for data\n",
        "train, val, test = TabularDataset.splits(path=part2_path , train='train.csv', validation=\"validation.csv\", test='test.csv',format='csv', \n",
        "                                  fields=[('label', label_field),('text', txt_field)], skip_header=True)\n",
        "\n",
        "# build the vocabulary on the training set only\n",
        "txt_field.build_vocab(train, min_freq=5)\n",
        "# label_field.build_vocab(train)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gyzYBCvVh7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make iterator for splits\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train, val, test), batch_sizes=(32, 256, 256),\n",
        "    sort_key=lambda x: len(x.text),sort_within_batch=True, device=device)\n",
        "# train_iter, test_iter = data.BucketIterator.splits((train, val), batch_size=30, \n",
        "                                                  #  sort_key=lambda x: len(x.sentence),sort_within_batch=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPPZJyDFOcYZ",
        "colab_type": "code",
        "outputId": "2fa56c40-cca7-40f2-ca0e-957d87022f7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "print(f'Number of training samples: {len(train.examples)}')\n",
        "print(f'Number of validation samples: {len(val.examples)}')\n",
        "print(f'Number of testing samples: {len(test.examples)}')\n",
        "\n",
        "print(f'Example of training data:\\n {vars(train.examples[0])}\\n')\n",
        "print(f'Example of validation data:\\n {vars(val.examples[1])}\\n')\n",
        "print(f'Example of testing data:\\n {vars(test.examples[1])}\\n')\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 40000\n",
            "Number of validation samples: 5000\n",
            "Number of testing samples: 3000\n",
            "Example of training data:\n",
            " {'label': '1', 'text': ['i', 'guess', 'those', 'who', 'have', 'been', 'in', 'a', 'one-sided', 'relationship', 'of', 'some', 'sort', 'before', 'will', 'be', 'able', 'identify', 'with', 'the', 'lead', 'character', 'minako', 'yuko', 'tanaka', ',', 'a', '50', 'year', 'old', 'woman', 'who', 'is', 'still', 'in', 'the', 'pink', 'of', 'good', 'health', ',', 'as', 'demonstrated', 'by', 'her', 'daily', ',', 'grinding', 'routine', 'of', 'waking', 'up', 'extremely', 'early', 'in', 'the', 'morning', 'to', 'prepare', 'for', 'her', 'milk', 'delivery', 'work', ',', 'where', 'she', 'has', 'to', 'lug', 'bottles', 'of', 'megmilk', 'in', 'a', 'bag', 'in', 'a', 'route', 'around', 'her', 'town', 'like', 'clockwork', ',', 'to', 'exchange', 'empty', 'bottles', 'for', 'full', 'ones', ',', 'and', 'to', 'collect', 'payment', 'and', 'issue', 'receipt', '.', 'and', 'there', \"'s\", 'always', 'be', 'that', 'one', 'delivery', 'stop', 'that', \"'s\", 'right', 'at', 'the', 'top', ',', 'needing', 'to', 'scale', 'a', 'long', 'flight', 'of', 'stairs', 'in', 'order', 'to', 'achieve', 'customer', 'satisfaction', '.', 'and', 'peculiar', 'enough', ',', 'that', 'stop', 'happened', 'to', 'be', 'a', 'stop', 'delivering', 'to', 'a', 'man', 'with', 'whom', 'she', 'has', 'been', 'in', 'love', 'with', 'for', 'almost', 'all', 'her', 'teenage', 'to', 'adult', 'life', ',', 'and', 'not', 'having', 'the', 'product', 'appreciated', ',', 'but', 'poured', 'down', 'the', 'sink', '.', 'having', 'gone', 'to', 'the', 'same', 'school', ',', 'we', 'see', 'that', 'they', \"'re\", 'not', 'talking', 'to', 'each', 'other', ',', 'and', 'in', 'their', 'daily', 'life', 'always', 'seem', 'so', 'close', 'physically', ',', 'but', 'yet', 'so', 'far', 'away', '.', 'there', \"'s\", 'no', 'eye', 'contact', ',', 'save', 'for', 'cursory', 'glances', 'by', 'chance', ',', 'and', 'little', 'acknowledgement', 'of', 'each', 'other', \"'s\", 'existence', '.', 'we', 'learn', 'that', 'they', 'share', 'a', 'past', 'that', 'probably', 'destroyed', 'all', 'notions', 'of', 'being', 'together', ',', 'where', 'clear', 'attraction', 'between', 'the', 'two', 'was', 'hampered', 'from', 'developing', 'further', 'by', 'the', 'earlier', 'generation', '.', 'while', 'i', 'thought', 'minako', 'was', 'an', 'interesting', 'woman', 'in', 'herself', ',', 'one', 'who', 'has', 'kept', 'her', 'feelings', 'suppressed', 'for', 'so', 'long', ',', 'one', 'can', 'only', 'wonder', 'what', 'kind', 'of', 'damage', 'it', 'would', 'do', '.', 'if', 'i', 'read', 'that', 'the', 'original', 'japanese', 'title', 'means', 'at', 'some', 'time', 'the', 'days', 'you', 'read', 'books', 'and', 'it', \"'s\", 'accurate', ',', 'i', 'felt', 'the', 'movie', 'had', 'a', 'wonderful', 'finale', 'with', 'that', 'shot', 'of', 'her', 'well', 'stocked', 'bookcase', ',', 'likely', 'alluding', 'to', 'the', 'fact', 'that', 'she', \"'s\", 'not', 'alone', 'after', 'all', ',', 'and', 'had', 'probably', 'fallen', 'back', 'on', 'her', 'crutch', 'of', 'sorts', 'to', 'deal', 'with', 'the', 'pain', 'of', 'being', 'alone', ',', 'and', 'back', 'to', 'a', 'lifestyle', 'which', 'she', 'had', 'already', 'been', 'accustomed', 'to', 'for', '50', 'years', '.', 'besides', 'immersing', 'herself', 'in', 'two', 'jobs', ',', 'she', 'has', 'those', 'books', 'which', 'serve', 'as', 'a', 'form', 'of', 'escapism', ',', 'and', 'occasionally', 'pens', 'little', 'sweet', 'nothings', 'to', 'song', 'dedication', 'shows', 'on', 'the', 'radio', '.', 'yuko', 'tanaka', 'did', 'a', 'commendable', 'job', 'as', 'the', 'emotionally', 'strong', 'woman', 'resigned', 'to', 'her', 'fate', 'and', 'her', 'decision', 'to', 'love', 'none', 'other', ',', 'her', 'object', 'of', 'affection', ',', 'takanashi', 'ittoku', 'kishibe', 'was', 'a', 'more', 'interesting', 'character', 'who', 'has', 'more', 'facets', '.', 'staying', 'true', 'to', 'marriage', 'vows', ',', 'he', 'spends', 'significant', 'amount', 'of', 'screen', 'time', 'looking', 'after', 'his', 'sickly', 'bedridden', 'wife', 'played', 'by', 'akiko', 'nishina', ',', 'while', 'juggling', 'with', 'his', 'job', 'of', 'social', 'welfare', 'in', 'the', 'children', \"'s\", 'affairs', 'department', 'in', 'city', 'hall', '.', 'i', 'felt', 'that', 'as', 'a', 'childless', 'couple', ',', 'the', 'job', 'provided', 'him', 'a', 'means', 'to', 'care', ',', 'not', 'for', 'his', 'own', ',', 'but', 'for', 'other', 'people', \"'s\", 'children', ',', 'the', 'troubled', 'ones', 'who', 'are', 'neglected', 'and', 'left', 'to', 'fend', 'for', 'themselves', '.', 'in', 'a', 'rare', 'moment', 'of', 'rage', ',', 'we', 'see', 'how', 'he', 'angrily', 'chides', 'such', 'wayward', 'parents', 'who', 'do', \"n't\", 'appreciate', 'and', 'wastes', 'their', 'children', \"'s\", 'lives', 'away', '.', 'the', 'story', 'by', 'kenji', 'aoki', 'provides', 'little', 'quirks', 'to', 'make', 'its', 'characters', 'appeal', 'and', 'successfully', 'attempted', 'to', 'provide', 'a', 'lot', 'more', 'glimpses', 'and', 'dimension', 'into', 'them', 'as', 'well', ',', 'such', 'as', 'how', 'takanashi', 'is', 'a', 'hopeless', 'haiku', 'poet', 'despite', 'being', 'a', 'member', 'of', 'the', 'haiku', 'club', ',', 'and', 'supporting', 'characters', 'such', 'as', 'the', 'aged', 'minagawa', 'couple', ',', 'where', 'masao', 'koichi', 'ueda', 'lent', 'some', 'comical', 'though', 'sad', 'moments', 'as', 'he', 'slowly', 'turned', 'senile', ',', 'while', 'wife', 'toshiko', 'misako', 'watanabe', 'narrates', 'and', 'brings', 'us', 'through', 'this', 'love', 'story', 'of', 'a', 'single', 'woman', 'at', '50', '.', 'even', 'akiko', 'nishina', \"'s\", 'performance', 'as', 'the', 'bedridden', 'wife', 'was', 'nothing', 'short', 'of', 'arresting', ',', 'with', 'her', 'character', \"'s\", 'enlightened', 'state', 'of', 'knowing', 'her', 'husband', \"'s\", 'past', ',', 'and', 'making', 'unselfish', ',', 'and', 'painful', 'decisions', 'in', 'her', 'sickly', 'state', '.', 'it', \"'s\", 'what', 'you', 'can', 'expect', 'from', 'a', 'typical', 'japanese', 'romantic', 'movie', ',', 'sans', 'young', ',', 'nubile', 'leads', 'as', 'star-crossed', 'lovers', ',', 'but', 'with', 'all', 'other', 'elements', 'in', 'place', 'such', 'as', 'romantic', 'set', 'ups', ',', 'love', 'songs', 'and', 'those', 'quintessential', 'restrained', 'but', 'affectionate', 'behaviour', '.', 'i', 'thought', 'the', 'story', 'was', 'in', 'danger', 'of', 'going', 'down', 'the', 'beaten', 'track', 'when', 'unrequited', 'love', 'gets', 'consummated', ',', 'but', 'director', 'akira', 'ogata', 'managed', 'to', 'steer', 'clear', 'of', 'the', 'usual', 'melodramatic', 'moments', 'in', 'such', 'stories', ',', 'though', 'the', 'story', 'did', 'call', 'for', 'some', 'obvious', 'plot', 'development', 'into', 'the', 'final', 'act', 'that', 'you', 'can', 'predict', ',', 'especially', 'if', 'you', \"'re\", 'already', 'way', 'past', 'your', 'romance', 'movie', '101', '.', 'not', 'being', 'your', 'average', 'lovey-dovey', 'story', ',', 'i', 'thought', 'the', 'milkwoman', 'told', 'a', 'strong', 'story', 'with', 'unrequited', 'love', 'as', 'a', 'central', 'theme', ',', 'and', 'frankly', 'a', 'recommended', 'romance', 'movie', 'though', 'told', 'at', 'a', 'measured', 'pace', 'if', 'you', \"'re\", 'in', 'the', 'mood', 'for', 'some', 'bittersweet', 'loving', ',', 'reminiscence', ',', 'and', 'seeking', 'to', 'live', 'without', 'regrets', '.']}\n",
            "\n",
            "Example of validation data:\n",
            " {'label': '0', 'text': ['alexandra', 'ripley', 'wrote', 'a', 'horrible', 'sequel', 'to', 'margaret', 'mitchell', \"'s\", 'masterpiece', 'book', 'published', 'in', 'the', '1930', \"'s\", '.', 'margaret', 'mitchell', \"'s\", 'heirs', 'sold', 'out', 'their', 'rights', 'and', 'for', 'big', 'bucks', 'allowed', 'alexandra', 'ripley', 'to', 'write', 'a', 'piece', 'of', 'junk', 'book', 'even', 'worse', 'than', 'barbara', 'cortland', 'romance', 'novels', '.', 'i', 'was', 'a', 'huge', 'fan', 'of', 'margaret', 'mitchells', 'book', 'and', 'the', 'fake', 'sequel', 'by', 'alexandra', 'ripley', 'was', 'written', 'just', 'to', 'cash', 'in', 'for', 'money', '.', 'although', 'i', 'always', 'admired', 'the', 'acting', 'talent', 'of', 'joanne', 'kilmer', 'and', 'timothy', 'dalton', ',', 'this', 'is', 'a', 'really', 'terrible', 'film', '.', 'the', 'script', 'is', 'horrible', 'and', 'full', 'of', 'clich', 's.', 'ann', 'margarets', 'cameo', 'as', 'belle', 'watling', 'is', 'so', 'awful', 'i', 'wanted', 'to', 'slap', 'her', '.', 'the', 'only', 'worthwhile', 'thing', 'in', 'the', 'movie', 'is', 'sean', 'bean', 'who', 'gives', 'a', 'masterful', 'bravura', 'performance', 'as', 'the', 'sexy', ',', 'feral', 'villain', '-', 'lord', 'fenton', '.', 'sean', 'bean', \"'s\", 'performance', 'is', 'along', 'the', 'lines', 'of', 'the', 'man', 'you', 'love', 'to', 'hate', 'and', 'portrays', 'an', 'unsafe', 'sex', 'symbol', '.', 'but', 'sean', 'bean', 'is', 'only', 'in', 'the', 'first', 'half', 'of', 'the', 'movie', 'so', 'you', 'then', 'have', 'to', 'be', 'tormented', 'with', 'watching', 'an', 'incredibly', 'long', '6', 'hour', 'movie', 'with', 'an', 'insufferably', 'boring', 'script', '.', 'do', \"n't\", 'waste', 'your', 'money', 'on', 'this', 'film', ',', 'unless', 'you', 'are', 'a', 'hard', 'core', 'sean', 'bean', 'fan', 'and', 'just', 'watch', 'it', 'for', 'his', 'wonderful', 'performance', '.']}\n",
            "\n",
            "Example of testing data:\n",
            " {'label': '0', 'text': ['almost', 'from', 'the', 'word', 'go', 'this', 'film', 'is', 'poor', 'and', 'lacking', 'conviction', 'but', 'then', 'again', 'most', 'people', 'would', 'struggle', 'to', 'show', 'commitment', 'to', 'a', 'script', 'as', 'uninspiring', 'as', 'this', '.', 'the', 'dialogue', 'really', 'does', 'not', 'flow', 'and', 'sometimes', 'as', 'in', 'this', 'case', 'more', 'is', 'less', 'or', 'should', 'have', 'been', '.', 'this', 'is', 'also', 'backed-up', 'by', 'odd', 'scenes', 'e.g', '.', 'the', 'cemetry', 'slow-motion', 'walk', 'that', 'you', 'think', 'might', 'lead', 'somewhere', 'but', 'only', 'seem', 'to', 'waste', 'a', 'few', 'more', 'seconds', 'of', 'your', 'life', '.', 'the', 'plot', 'is', 'a', 'strange', 'combination', 'of', 'gangster', 'situation', 'comedy', 'which', 'i', 'am', 'sure', 'seemed', 'a', 'good', 'idea', 'at', 'the', 'time', 'but', 'if', 'ever', 'there', 'was', 'a', 'case', 'for', 'someone', 'needing', 'to', 'be', 'honest', 'with', 'the', 'scriptwriter', 'then', 'here', 'was', 'it', '.', 'martin', 'freeman', 'is', 'okay', 'but', 'then', 'he', 'seems', 'to', 'have', 'one', 'character', 'which', 'always', 'plays', 'so', 'i', 'am', 'beginning', 'to', 'wonder', 'if', 'he', 'was', 'given', 'a', 'script', 'or', 'just', 'filmed', 'and', 'told', 'to', 'react', 'as', 'normal', '.', 'finally', '-', 'humour', '.', 'this', 'reminds', 'me', 'of', 'the', \"'python\", 'i', 'think', 'quote', 'about', 'shakespere', ',', 'of', 'his', \"'comedies\", \"'\", '-', 'if', 'he', 'had', 'meant', 'it', 'to', 'be', 'humorous', 'he', 'would', 'have', 'put', 'a', 'joke', 'in', 'it', '.', 'well', 'i', 'did', \"n't\", 'see', 'one', '.', 'do', \"n't\", 'waste', 'your', 'time', '-', 'i', 'did', 'because', 'i', 'was', 'watching', 'it', 'with', 'a', 'friend', 'and', 'kept', 'hoping', 'that', 'it', 'was', 'going', 'to', 'get', 'better', '.', 'it', 'did', \"n't\", '.']}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1oFVp0zNhxF",
        "colab_type": "code",
        "outputId": "30ea521f-e17f-4529-f997-5949d3e39b33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "_, batch = next(enumerate(train_iter))\n",
        "print('label tensor', batch.label.shape)\n",
        "print(batch.label)\n",
        "print()\n",
        "sent, sent_len = batch.text\n",
        "print('sentence length tensor', sent_len.shape)\n",
        "print(sent_len)\n",
        "print()\n",
        "print('sentence tensor', sent.shape)\n",
        "print(sent)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label tensor torch.Size([32])\n",
            "tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1])\n",
            "\n",
            "sentence length tensor torch.Size([32])\n",
            "tensor([219, 219, 219, 219, 218, 218, 218, 218, 218, 218, 218, 217, 217, 217,\n",
            "        217, 217, 217, 217, 217, 217, 217, 217, 216, 216, 216, 216, 216, 216,\n",
            "        216, 216, 216, 216])\n",
            "\n",
            "sentence tensor torch.Size([32, 219])\n",
            "tensor([[  53,   38, 1155,  ...,  907,   22,    4],\n",
            "        [  53,   15,  251,  ...,    6,  170,   33],\n",
            "        [  12,  221,   13,  ...,   45, 2243,   33],\n",
            "        ...,\n",
            "        [  56,   23,  111,  ...,    1,    1,    1],\n",
            "        [  42,    2, 3827,  ...,    1,    1,    1],\n",
            "        [   2, 1423,    7,  ...,    1,    1,    1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uztDy6U6ee8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_checkpoint(save_path, model, optimizer, val_loss):\n",
        "    if save_path==None:\n",
        "        return\n",
        "    save_path = save_path \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'val_loss': val_loss}\n",
        "\n",
        "    torch.save(state_dict, save_path)\n",
        "\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_checkpoint(model, optimizer, save_path):\n",
        "    state_dict = torch.load(save_path)\n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "    val_loss = state_dict['val_loss']\n",
        "    print(f'Model loaded from <== {save_path}')\n",
        "    \n",
        "    return val_loss\n",
        "\n",
        "\n",
        "def TRAIN(model, train_loader, valid_loader,  num_epochs, eval_every, total_step, criterion, optimizer, val_loss, device, save_name):\n",
        "    loss_his = {'train' : [] , 'val' : []}\n",
        "    acc_his = {'train' : [] , 'val' : []}\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    running_cor = 0\n",
        "    running_num = 0\n",
        "    global_step = 0\n",
        "    if val_loss==None:\n",
        "        best_val_loss = float(\"Inf\")  \n",
        "    else: \n",
        "        best_val_loss=val_loss\n",
        "    \n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        for i, X in enumerate(train_loader):\n",
        "            model.train()\n",
        "\n",
        "            inputs = (X.text[0]).to(device)\n",
        "            sent_len = (X.text[1]).to(device)\n",
        "            labels = X.label.to(device)\n",
        "\n",
        "            '''Training of the model'''\n",
        "            # Forward pass\n",
        "            outputs = model(inputs,sent_len)\n",
        "            if(not(  (outputs.reshape(-1) >= 0).all() and (outputs.reshape(-1) <= 1).all() ) ):\n",
        "              print(\"Something Wrong in the model output!\")\n",
        "              return X, outputs\n",
        "            loss = criterion(outputs, labels.reshape(-1,1).float())\n",
        "            predicts = torch.where(outputs.reshape(-1) > 0.5, torch.ones(1).to(device) ,torch.zeros(1).to(device))\n",
        "            running_cor += torch.sum(predicts == labels)\n",
        "            running_num += len(labels)\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            global_step += 1\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            '''Evaluating the model every x steps'''\n",
        "            if global_step % eval_every == 0:\n",
        "                with torch.no_grad():\n",
        "                    model.eval()\n",
        "                    val_running_loss = 0.0\n",
        "                    val_running_cor = 0\n",
        "                    val_running_num = 0\n",
        "                    for val_X in valid_loader:\n",
        "                        val_labels = val_X.label\n",
        "                        val_outputs = model(val_X.text[0], val_X.text[1])\n",
        "                        val_loss = criterion(val_outputs, val_labels.reshape(-1,1).float())\n",
        "                        val_running_loss += val_loss.item()\n",
        "                        predicts = torch.where(val_outputs.reshape(-1) > 0.5,torch.ones(1).to(device),torch.zeros(1).to(device))\n",
        "                        val_running_cor += torch.sum(predicts == val_labels)\n",
        "                        val_running_num += len(val_labels)\n",
        "                    average_train_loss = running_loss / eval_every\n",
        "                    average_val_loss = val_running_loss / len(valid_loader)\n",
        "\n",
        "                    average_train_acc = running_cor / float(running_num)\n",
        "                    # print(val_running_cor,float(len(valid_loader)))\n",
        "                    average_val_acc = val_running_cor / float(val_running_num)\n",
        "\n",
        "                    print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}' \n",
        "                          .format(epoch+1, num_epochs, global_step, total_step, average_train_loss, average_val_loss))\n",
        "                    print(f'Train Acc : {average_train_acc} ; Validation Acc {average_val_acc}')\n",
        "\n",
        "                    loss_his['train'].append(average_train_loss)\n",
        "                    loss_his['val'].append(average_val_loss)\n",
        "                    acc_his['train'].append(average_train_acc)\n",
        "                    acc_his['val'].append(average_val_acc)\n",
        "\n",
        "\n",
        "                    running_loss = 0.0\n",
        "                    running_num = 0\n",
        "                    running_cor = 0\n",
        "                    if average_val_loss < best_val_loss:\n",
        "                        best_val_loss = average_val_loss\n",
        "                        save_checkpoint(save_name, model, optimizer, best_val_loss)\n",
        "                    \n",
        "\n",
        "            \n",
        "    print('Finished Training')\n",
        "    return loss_his, acc_his"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gag4RuNVmDd7",
        "colab_type": "text"
      },
      "source": [
        "##RNN Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmtd54SQV_Zv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Text_RNN(nn.Module):\n",
        "    def __init__(self, n_vocab, embedding_dim, n_hidden, n_layers, dropout, out_features):\n",
        "        super(Text_RNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.emb = nn.Embedding(n_vocab, embedding_dim)\n",
        "        self.rnn = nn.RNN(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=n_hidden,\n",
        "                num_layers=n_layers,\n",
        "                batch_first=True,\n",
        "                dropout = dropout,\n",
        "                nonlinearity = 'relu'\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(n_hidden, out_features)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, sent, sent_len):\n",
        "        # sent: batch_size, max_sent_len\n",
        "        # sent_len: batch_size\n",
        "        # print(sent.dtype)\n",
        "        sent_emb = self.emb(sent)  #batch_size, max_sent_len, embedding_dim\n",
        "        # method 1\n",
        "        # rnn_outputs, hidden = self.rnn(sent_emb)\n",
        "        # print(\"here\", outputs)\n",
        "        # method 2, pack the input sequence, more computationally efficient\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(sent_emb, sent_len, batch_first=True)\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "        rnn_outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "        dropout = self.dropout(rnn_outputs[:,-1,:])\n",
        "        outputs = self.fc(dropout)\n",
        "        # outputs = self.sigmoid(outputs)\n",
        "\n",
        "\n",
        "        return outputs\n",
        "        #output: batch_size, max_sent_len, n_hidden\n",
        "        #hidden: n_layer, batch_size, n_hidden \n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6bDZBIuOmc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import Adam\n",
        "n_hidden = 128\n",
        "n_layers = 1\n",
        "dropout = 0.5\n",
        "embedding_dim = 64\n",
        "out_features = 1\n",
        "n_vocab =len(txt_field.vocab)\n",
        "\n",
        "\n",
        "# build model\n",
        "\n",
        "model = Text_RNN(n_vocab, embedding_dim, n_hidden, n_layers, dropout, out_features).to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2ZoAu6sdbzG",
        "colab_type": "code",
        "outputId": "105b51ab-2806-43ce-e27d-d3c91e6628f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# print model achitecture and total number of parameters\n",
        "\n",
        "def count_parameters(model):\n",
        "    temp = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'The model architecture:\\n\\n', model)\n",
        "    print(f'\\nThe model has {temp:,} trainable parameters')\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "        \n",
        "model.apply(init_weights)\n",
        "count_parameters(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model architecture:\n",
            "\n",
            " Text_RNN(\n",
            "  (emb): Embedding(38344, 64)\n",
            "  (rnn): RNN(64, 128, batch_first=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "The model has 2,478,977 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3yyw5tFhAad",
        "colab_type": "code",
        "outputId": "b0dfdc4f-f7b1-46ad-ab13-b9fb4cd21399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "n_epochs = 10\n",
        "val_loss = None\n",
        "save_name = 'rnn_model'\n",
        "eval_every =  len(train_iter)//5\n",
        "total_step = len(train_iter)*n_epochs\n",
        "criterion = nn.BCELoss()\n",
        "lr = 0.005\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "#start training\n",
        "loss_his, acc_his = TRAIN(model,train_iter, val_iter,  n_epochs, \n",
        "      eval_every, total_step, criterion, optimizer, val_loss, device, save_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [250/12500], Train Loss: 0.6958, Valid Loss: 0.6925\n",
            "Train Acc : 0.49387502670288086 ; Validation Acc 0.49939998984336853\n",
            "Model saved to ==> rnn_model\n",
            "Epoch [1/10], Step [500/12500], Train Loss: 0.6948, Valid Loss: 0.6926\n",
            "Train Acc : 0.5021250247955322 ; Validation Acc 0.5001999735832214\n",
            "Epoch [1/10], Step [750/12500], Train Loss: 0.7030, Valid Loss: 0.6928\n",
            "Train Acc : 0.5062500238418579 ; Validation Acc 0.5008000135421753\n",
            "Epoch [1/10], Step [1000/12500], Train Loss: 0.6945, Valid Loss: 0.6931\n",
            "Train Acc : 0.499625027179718 ; Validation Acc 0.49879997968673706\n",
            "Epoch [1/10], Step [1250/12500], Train Loss: 0.6931, Valid Loss: 0.6932\n",
            "Train Acc : 0.5007500052452087 ; Validation Acc 0.4989999830722809\n",
            "Epoch [2/10], Step [1500/12500], Train Loss: 0.7554, Valid Loss: 0.6928\n",
            "Train Acc : 0.5003750324249268 ; Validation Acc 0.5023999810218811\n",
            "Epoch [2/10], Step [1750/12500], Train Loss: 0.6908, Valid Loss: 0.6925\n",
            "Train Acc : 0.49925002455711365 ; Validation Acc 0.5004000067710876\n",
            "Epoch [2/10], Step [2000/12500], Train Loss: 0.6957, Valid Loss: 0.6922\n",
            "Train Acc : 0.5107499957084656 ; Validation Acc 0.503600001335144\n",
            "Model saved to ==> rnn_model\n",
            "Epoch [2/10], Step [2250/12500], Train Loss: 0.6908, Valid Loss: 0.6930\n",
            "Train Acc : 0.5170000195503235 ; Validation Acc 0.5\n",
            "Epoch [2/10], Step [2500/12500], Train Loss: 0.6941, Valid Loss: 0.6924\n",
            "Train Acc : 0.5062500238418579 ; Validation Acc 0.5004000067710876\n",
            "Epoch [3/10], Step [2750/12500], Train Loss: 0.6909, Valid Loss: 0.6926\n",
            "Train Acc : 0.5070000290870667 ; Validation Acc 0.4997999966144562\n",
            "Epoch [3/10], Step [3000/12500], Train Loss: 0.6875, Valid Loss: 0.6938\n",
            "Train Acc : 0.51500004529953 ; Validation Acc 0.5009999871253967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX-9PiOVO-4V",
        "colab_type": "code",
        "outputId": "37786afb-ae4a-455d-c8c4-aefe1d608b7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# print(loss_his.text[0][0])\n",
        "# print(model(loss_his.text[0],loss_his.text[1]))\n",
        "train.examples[0].text[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiII1LQahSDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_graph(loss_his):\n",
        "  y1 = loss_his['train']\n",
        "  y2 = loss_his['val']\n",
        "  x = range(len(loss_his[0]))\n",
        "\n",
        "  print(best_val_loss)\n",
        "  plt.plot(x,y1,label = \"train loss\")\n",
        "  plt.plot(x,y2,label = \"val loss\")\n",
        "\n",
        "\n",
        "  plt.title(\"Loss Graph\")\n",
        "  plt.legend()  \n",
        "  plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP02ij_Njd_3",
        "colab_type": "text"
      },
      "source": [
        "##Different RNN Setting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v94vWZWvlg2m",
        "colab_type": "text"
      },
      "source": [
        "#####1. GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7-TVFade_gO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Text_GRU(nn.Module):\n",
        "    def __init__(self, n_vocab, embedding_dim, n_hidden, n_layers, dropout, out_features):\n",
        "        super(Text_GRU, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.emb = nn.Embedding(n_vocab, embedding_dim)\n",
        "        self.rnn = nn.GRU(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=n_hidden,\n",
        "                num_layers=n_layers,\n",
        "                batch_first=True,\n",
        "                dropout = dropout\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(n_hidden, out_features)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, sent, sent_len):\n",
        "        # sent: batch_size, max_sent_len\n",
        "        # sent_len: batch_size\n",
        "        # print(sent.dtype)\n",
        "        sent_emb = self.emb(sent)  #batch_size, max_sent_len, embedding_dim\n",
        "        # method 1\n",
        "        # rnn_outputs, hidden = self.rnn(sent_emb)\n",
        "        # print(\"here\", outputs)\n",
        "        # method 2, pack the input sequence, more computationally efficient\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(sent_emb, sent_len, batch_first=True)\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "        rnn_outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "        dropout = self.dropout(rnn_outputs[:,-1,:])\n",
        "        outputs = self.fc(dropout)\n",
        "        outputs = self.sigmoid(outputs)\n",
        "\n",
        "\n",
        "        return outputs\n",
        "        #output: batch_size, max_sent_len, n_hidden\n",
        "        #hidden: n_layer, batch_size, n_hidden \n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZOWSGPhe_dm",
        "colab_type": "code",
        "outputId": "6a60fe12-177e-4043-f2fd-b644630f5543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from torch.optim import Adam\n",
        "n_hidden = 128\n",
        "n_layers = 1\n",
        "dropout = 0.5\n",
        "embedding_dim = 64\n",
        "out_features = 1\n",
        "n_vocab =len(txt_field.vocab)\n",
        "\n",
        "\n",
        "# build model\n",
        "\n",
        "model_1 = Text_GRU(n_vocab, embedding_dim, n_hidden, n_layers, dropout, out_features).to(device)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euAFnVsce_a_",
        "colab_type": "code",
        "outputId": "4b109b00-b76d-4324-8711-25cd4a350607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "n_epochs = 10\n",
        "val_loss = None\n",
        "save_name = 'gru_model'\n",
        "eval_every = len(train_iter)//5\n",
        "total_step = len(train_iter)*n_epochs\n",
        "criterion = nn.BCELoss()\n",
        "# lr = 0.005\n",
        "optimizer = Adam(model_1.parameters())\n",
        "#start training\n",
        "loss_1_his, acc_1_his = TRAIN(model_1,train_iter,  val_iter,  n_epochs, \n",
        "      eval_every, total_step, criterion, optimizer, val_loss, device, save_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-4ead479c441b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m loss_1_his, acc_1_his = TRAIN(model_1,train_iter,  val_iter,  n_epochs, \n\u001b[0;32m---> 11\u001b[0;31m       eval_every, total_step, criterion, optimizer, val_loss, device, save_name)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-323014af0006>\u001b[0m in \u001b[0;36mTRAIN\u001b[0;34m(model, train_loader, valid_loader, num_epochs, eval_every, total_step, criterion, optimizer, val_loss, device, save_name)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlLNjpfde_YW",
        "colab_type": "code",
        "outputId": "5ecb8e22-4eef-4bdf-ad64-c49e24faa6af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "plot_graph(loss_1_his)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-f4793978cf11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_1_his\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-0dff82adfd1e>\u001b[0m in \u001b[0;36mplot_graph\u001b[0;34m(loss_his)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_his\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_his\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_his\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_his\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Batch' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIfMJFFrlTEN",
        "colab_type": "text"
      },
      "source": [
        "####2. LTSM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lDMQNKQe_U3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Text_LSTM(nn.Module):\n",
        "    def __init__(self, n_vocab, embedding_dim, n_hidden, n_layers, dropout, out_features, bidirectional):\n",
        "        super(Text_LSTM, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.emb = nn.Embedding(n_vocab, embedding_dim)\n",
        "        self.rnn = nn.LSTM(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=n_hidden,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional = bidirectional,\n",
        "                batch_first=True,\n",
        "                dropout = dropout\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(n_hidden, out_features)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, sent, sent_len):\n",
        "        # sent: batch_size, max_sent_len\n",
        "        # sent_len: batch_size\n",
        "        # print(sent.dtype)\n",
        "        sent_emb = self.emb(sent)  #batch_size, max_sent_len, embedding_dim\n",
        "        # method 1\n",
        "        # rnn_outputs, hidden = self.rnn(sent_emb)\n",
        "        # print(\"here\", outputs)\n",
        "        # method 2, pack the input sequence, more computationally efficient\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(sent_emb, sent_len, batch_first=True)\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "        rnn_outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "        dropout = self.dropout(rnn_outputs[:,-1,:])\n",
        "        outputs = self.fc(dropout)\n",
        "        outputs = self.sigmoid(outputs)\n",
        "\n",
        "\n",
        "        return outputs\n",
        "        #output: batch_size, max_sent_len, n_hidden\n",
        "        #hidden: n_layer, batch_size, n_hidden \n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20SPgvzwtjTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "b7ce40a4-82d4-4aa1-967d-658cb1516a31"
      },
      "source": [
        "n_hidden = 128\n",
        "n_layers = 1\n",
        "dropout = 0.5\n",
        "embedding_dim = 64\n",
        "out_features = 1\n",
        "n_vocab =len(txt_field.vocab)\n",
        "\n",
        "\n",
        "# build model\n",
        "model_2 = Text_LSTM(n_vocab, embedding_dim, n_hidden, n_layers, dropout, out_features, False).to(device)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYwBN7iLe_R8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "b45936de-40ab-4753-e479-b255fa9069fd"
      },
      "source": [
        "from torch.optim import Adam\n",
        "n_epochs = 10\n",
        "val_loss = None\n",
        "save_name = 'lstm_model'\n",
        "eval_every = len(train_iter)//5\n",
        "total_step = len(train_iter)*n_epochs\n",
        "criterion = nn.BCELoss()\n",
        "# lr = 0.005\n",
        "optimizer = Adam(model_2.parameters())\n",
        "#start training\n",
        "loss_2_his, acc_2_his = TRAIN(model_2,train_iter,  val_iter,  n_epochs, \n",
        "      eval_every, total_step, criterion, optimizer, val_loss, device, save_name)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtC4_X-ce_Pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc_6rz1ve93L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}